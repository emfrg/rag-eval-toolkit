{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
        "\n",
        "For an introduction to RAG, you can check [this other cookbook](rag_zephyr_langchain)!\n",
        "\n",
        "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
        "So let's see how to evaluate our RAG system.\n",
        "\n",
        "### Evaluating RAG performance\n",
        "\n",
        "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
        "\n",
        "For our evaluation pipeline, we will need:\n",
        "1. An evaluation dataset with question - answer couples (QA couples)\n",
        "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
        "\n",
        "‚û°Ô∏è It turns out, we can use LLMs to help us all along the way!\n",
        "1. The evaluation dataset will be synthetically generated by an LLM ü§ñ, and questions will be filtered out by other LLMs ü§ñ\n",
        "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ü§ñ will then perform the evaluation on this synthetic dataset.\n",
        "\n",
        "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCKBvOcp9jVK"
      },
      "outputs": [],
      "source": [
        "# !pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k_lJFbYm9jVL"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4ff_mr3lBMUc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "801cafa190dd42098d8cea38ada2178d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeW8P62J9jVM"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YRbm5tNF9jVM"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset(\n",
        "    \"m-ric/huggingface_doc\", split=\"train\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'source'],\n",
            "    num_rows: 2647\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation\n",
        "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
        "\n",
        "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3gTOlRKO9jVM"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20ddb6257eba40dd983666e39dea98f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "langchain_docs = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in langchain_docs:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "We use [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thank you for providing the test context! How can I assist you further?\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Create the LLM client\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "\n",
        "def call_llm(llm_client, prompt: str):\n",
        "    response = llm_client.invoke(\n",
        "        [\n",
        "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "            HumanMessage(content=prompt),\n",
        "        ]\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "\n",
        "print(call_llm(llm, \"This is a test context\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hIM_DJRo9jVN"
      },
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a factoid question and an answer given a context.\n",
        "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
        "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Factoid question: (your factoid question)\n",
        "Answer: (your answer to the factoid question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 30 QA couples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:35<00:00,  1.17s/it]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "N_GENERATIONS = 30  # keep it low for testing\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(\n",
        "    random.sample(docs_processed, min(N_GENERATIONS, len(docs_processed)))\n",
        "):\n",
        "    output_QA_couple = call_llm(\n",
        "        llm, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    )\n",
        "    try:\n",
        "        question = (\n",
        "            output_QA_couple.split(\"Factoid question: \")[-1]\n",
        "            .split(\"Answer: \")[0]\n",
        "            .strip()\n",
        "        )\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1].strip()\n",
        "        assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata.get(\"source\"),\n",
        "            }\n",
        "        )\n",
        "    except Exception:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aUlOUDv59jVN",
        "outputId": "c9634fdb-2a7f-43a6-c4eb-e60b166b8238"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;div class=\"flex justify-center\"&gt;\\n&lt;img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg\" alt=\"IOB1 vs IOB2 format\"/&gt;\\n&lt;img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg\" alt=\"IOB1 vs IOB2 format\"/&gt;\\n&lt;/div&gt;\\n\\nWith this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we can just grab the score and label of each token that was not classified as `O`:\\n\\n```py\\nresults = []\\ntokens = inputs.tokens()\\n\\nfor idx, pred in enumerate(predictions):\\n    label = model.config.id2label[pred]\\n    if label != \"O\":\\n        results.append(\\n            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\\n        )\\n\\nprint(results)\\n```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\\n {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},\\n {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},\\n {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},\\n {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},\\n {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},\\n {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},\\n {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]\\n```\\n\\nThis is very similar to what we had before, with one exception: the pipeline also gave us information about the `start` and `end` of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set `return_offsets_mapping=True` when we apply the tokenizer to our inputs:\\n\\n```py\\ninputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\\ninputs_with_offsets[\"offset_mapping\"]\\n```</td>\n",
              "      <td>What does setting return_offsets_mapping=True do when applying the tokenizer to inputs?</td>\n",
              "      <td>It provides the start and end offsets of each token in the original sentence.</td>\n",
              "      <td>huggingface/course/blob/main/chapters/en/chapter6/3.mdx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Being similar to LoRA, IA3 carries many of the same advantages: \\n\\n* IA3 makes fine-tuning more efficient by drastically reducing the number of trainable parameters. (For T0, an IA3 model only has about 0.01% trainable parameters, while even LoRA has &gt; 0.1%)\\n* The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable IA3 models for various downstream tasks built on top of them.\\n* Performance of models fine-tuned using IA3 is comparable to the performance of fully fine-tuned models.\\n* IA3 does not add any inference latency because adapter weights can be merged with the base model.\\n\\nIn principle, IA3 can be applied to any subset of weight matrices in a neural network to reduce the number of trainable\\nparameters. Following the authors' implementation, IA3 weights are added to the key, value and feedforward layers\\nof a Transformer model. To be specific, for transformer models, IA3 weights are added to the outputs of key and value layers, and to the input of the second feedforward layer\\nin each transformer block.\\n\\nGiven the target layers for injecting IA3 parameters, the number of trainable parameters\\ncan be determined based on the size of the weight matrices.\\n\\n\\n## Common IA3 parameters in PEFT\\n\\nAs with other methods supported by PEFT, to fine-tune a model using IA3, you need to:\\n\\n1. Instantiate a base model.\\n2. Create a configuration (`IA3Config`) where you define IA3-specific parameters.\\n3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.\\n4. Train the `PeftModel` as you normally would train the base model.\\n\\n`IA3Config` allows you to control how IA3 is applied to the base model through the following parameters:</td>\n",
              "      <td>To which layers of a Transformer model are IA3 weights added?</td>\n",
              "      <td>IA3 weights are added to the outputs of the key and value layers, and to the input of the second feedforward layer in each transformer block.</td>\n",
              "      <td>huggingface/peft/blob/main/docs/source/conceptual_guides/ia3.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Here's our chat function:\\n\\n```python\\nimport random\\n\\ndef random_response(message, history):\\n    return random.choice([\"Yes\", \"No\"])\\n```\\n\\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the web interface:\\n\\n```python\\nimport gradio as gr\\n\\ngr.ChatInterface(random_response).launch()\\n```\\n\\nThat's it! Here's our running demo, try it out:\\n\\n$demo_chatinterface_random_response\\n\\n## Another example using the user's input and history\\n\\nOf course, the previous example was very simplistic, it didn't even take user input or the previous history into account! Here's another simple example showing how to incorporate a user's input as well as the history.\\n\\n```python\\nimport random\\nimport gradio as gr\\n\\ndef alternatingly_agree(message, history):\\n    if len(history) % 2 == 0:\\n        return f\"Yes, I do think that '{message}'\"\\n    else:\\n        return \"I don't think so\"\\n\\ngr.ChatInterface(alternatingly_agree).launch()\\n```\\n\\n## Streaming chatbots\\n\\nIf in your chat function, you use `yield` to generate a sequence of responses, you'll end up with a streaming chatbot. It's that simple!\\n\\n```python\\nimport time\\nimport gradio as gr\\n\\ndef slow_echo(message, history):\\n    for i in range(len(message)):\\n        time.sleep(0.3)\\n        yield \"You typed: \" + message[: i+1]\\n\\ngr.ChatInterface(slow_echo).launch()\\n```\\n\\nNotice that we've [enabled queuing](/guides/key-features#queuing), which is required to use generator functions. While the response is streaming, the \"Submit\" button turns into a \"Stop\" button that can be used to stop the generator function. You can customize the appearance and behavior of the \"Stop\" button using the `stop_btn` parameter.\\n\\n## Customizing your chatbot\\n\\nIf you're familiar with Gradio's `Interface` class, the `gr.ChatInterface` includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:</td>\n",
              "      <td>What Python library is used to create the chat interface in the provided examples?</td>\n",
              "      <td>gradio</td>\n",
              "      <td>gradio-app/gradio/blob/main/guides/04_chatbots/01_creating-a-chatbot-fast.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwise separable convolution layers](https://paperswithcode.com/method/depthwise-separable-convolution).\\n\\nThe weights from this model were ported from [Tensorflow/Models](https://github.com/tensorflow/models).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('xception', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert('RGB')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\\n```</td>\n",
              "      <td>What type of convolutional layers does the Xception neural network architecture use exclusively?</td>\n",
              "      <td>Depthwise separable convolution layers</td>\n",
              "      <td>huggingface/pytorch-image-models/blob/main/docs/models/xception.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>. To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.</td>\n",
              "      <td>What function is used to convert logits into probabilities in the described pipeline?</td>\n",
              "      <td>SoftMax</td>\n",
              "      <td>huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          context  \\\n",
              "0                                                             <div class=\"flex justify-center\">\\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg\" alt=\"IOB1 vs IOB2 format\"/>\\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg\" alt=\"IOB1 vs IOB2 format\"/>\\n</div>\\n\\nWith this map, we are ready to reproduce (almost entirely) the results of the first pipeline -- we can just grab the score and label of each token that was not classified as `O`:\\n\\n```py\\nresults = []\\ntokens = inputs.tokens()\\n\\nfor idx, pred in enumerate(predictions):\\n    label = model.config.id2label[pred]\\n    if label != \"O\":\\n        results.append(\\n            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\\n        )\\n\\nprint(results)\\n```\\n\\n```python out\\n[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\\n {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},\\n {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},\\n {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},\\n {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},\\n {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},\\n {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},\\n {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]\\n```\\n\\nThis is very similar to what we had before, with one exception: the pipeline also gave us information about the `start` and `end` of each entity in the original sentence. This is where our offset mapping will come into play. To get the offsets, we just have to set `return_offsets_mapping=True` when we apply the tokenizer to our inputs:\\n\\n```py\\ninputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\\ninputs_with_offsets[\"offset_mapping\"]\\n```   \n",
              "1                                                                                                                                                                                                                                                                                        Being similar to LoRA, IA3 carries many of the same advantages: \\n\\n* IA3 makes fine-tuning more efficient by drastically reducing the number of trainable parameters. (For T0, an IA3 model only has about 0.01% trainable parameters, while even LoRA has > 0.1%)\\n* The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable IA3 models for various downstream tasks built on top of them.\\n* Performance of models fine-tuned using IA3 is comparable to the performance of fully fine-tuned models.\\n* IA3 does not add any inference latency because adapter weights can be merged with the base model.\\n\\nIn principle, IA3 can be applied to any subset of weight matrices in a neural network to reduce the number of trainable\\nparameters. Following the authors' implementation, IA3 weights are added to the key, value and feedforward layers\\nof a Transformer model. To be specific, for transformer models, IA3 weights are added to the outputs of key and value layers, and to the input of the second feedforward layer\\nin each transformer block.\\n\\nGiven the target layers for injecting IA3 parameters, the number of trainable parameters\\ncan be determined based on the size of the weight matrices.\\n\\n\\n## Common IA3 parameters in PEFT\\n\\nAs with other methods supported by PEFT, to fine-tune a model using IA3, you need to:\\n\\n1. Instantiate a base model.\\n2. Create a configuration (`IA3Config`) where you define IA3-specific parameters.\\n3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.\\n4. Train the `PeftModel` as you normally would train the base model.\\n\\n`IA3Config` allows you to control how IA3 is applied to the base model through the following parameters:   \n",
              "2                                                        Here's our chat function:\\n\\n```python\\nimport random\\n\\ndef random_response(message, history):\\n    return random.choice([\"Yes\", \"No\"])\\n```\\n\\nNow, we can plug this into `gr.ChatInterface()` and call the `.launch()` method to create the web interface:\\n\\n```python\\nimport gradio as gr\\n\\ngr.ChatInterface(random_response).launch()\\n```\\n\\nThat's it! Here's our running demo, try it out:\\n\\n$demo_chatinterface_random_response\\n\\n## Another example using the user's input and history\\n\\nOf course, the previous example was very simplistic, it didn't even take user input or the previous history into account! Here's another simple example showing how to incorporate a user's input as well as the history.\\n\\n```python\\nimport random\\nimport gradio as gr\\n\\ndef alternatingly_agree(message, history):\\n    if len(history) % 2 == 0:\\n        return f\"Yes, I do think that '{message}'\"\\n    else:\\n        return \"I don't think so\"\\n\\ngr.ChatInterface(alternatingly_agree).launch()\\n```\\n\\n## Streaming chatbots\\n\\nIf in your chat function, you use `yield` to generate a sequence of responses, you'll end up with a streaming chatbot. It's that simple!\\n\\n```python\\nimport time\\nimport gradio as gr\\n\\ndef slow_echo(message, history):\\n    for i in range(len(message)):\\n        time.sleep(0.3)\\n        yield \"You typed: \" + message[: i+1]\\n\\ngr.ChatInterface(slow_echo).launch()\\n```\\n\\nNotice that we've [enabled queuing](/guides/key-features#queuing), which is required to use generator functions. While the response is streaming, the \"Submit\" button turns into a \"Stop\" button that can be used to stop the generator function. You can customize the appearance and behavior of the \"Stop\" button using the `stop_btn` parameter.\\n\\n## Customizing your chatbot\\n\\nIf you're familiar with Gradio's `Interface` class, the `gr.ChatInterface` includes many of the same arguments that you can use to customize the look and feel of your Chatbot. For example, you can:   \n",
              "3  Xception\\n\\n**Xception** is a convolutional neural network architecture that relies solely on [depthwise separable convolution layers](https://paperswithcode.com/method/depthwise-separable-convolution).\\n\\nThe weights from this model were ported from [Tensorflow/Models](https://github.com/tensorflow/models).\\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('xception', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert('RGB')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\\nurllib.request.urlretrieve(url, filename) \\nwith open(\"imagenet_classes.txt\", \"r\") as f:\\n    categories = [s.strip() for s in f.readlines()]\\n\\n# Print top categories per image\\ntop5_prob, top5_catid = torch.topk(probabilities, 5)\\nfor i in range(top5_prob.size(0)):\\n    print(categories[top5_catid[i]], top5_prob[i].item())\\n# prints class names and probabilities like:\\n# [('Samoyed', 0.6425196528434753), ('Pomeranian', 0.04062102362513542), ('keeshond', 0.03186424449086189), ('white wolf', 0.01739676296710968), ('Eskimo dog', 0.011717947199940681)]\\n```   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      . To make sense of those logits, we need to dig into the third and last step of the pipeline: post-processing. To convert logits into probabilities, we need to apply a SoftMax layer to them. As we can see, this transforms them into positive numbers that sum up to 1. The last step is to know which of those corresponds to the positive or the negative label. This is given by the id2label field of the model config. The first probabilities (index 0) correspond to the negative label, and the seconds (index 1) correspond to the positive label. This is how our classifier built with the pipeline function picked those labels and computed those scores. Now that you know how each steps works, you can easily tweak them to your needs.   \n",
              "\n",
              "                                                                                           question  \\\n",
              "0           What does setting return_offsets_mapping=True do when applying the tokenizer to inputs?   \n",
              "1                                     To which layers of a Transformer model are IA3 weights added?   \n",
              "2                What Python library is used to create the chat interface in the provided examples?   \n",
              "3  What type of convolutional layers does the Xception neural network architecture use exclusively?   \n",
              "4             What function is used to convert logits into probabilities in the described pipeline?   \n",
              "\n",
              "                                                                                                                                          answer  \\\n",
              "0                                                                  It provides the start and end offsets of each token in the original sentence.   \n",
              "1  IA3 weights are added to the outputs of the key and value layers, and to the input of the second feedforward layer in each transformer block.   \n",
              "2                                                                                                                                         gradio   \n",
              "3                                                                                                         Depthwise separable convolution layers   \n",
              "4                                                                                                                                        SoftMax   \n",
              "\n",
              "                                                                        source_doc  \n",
              "0                          huggingface/course/blob/main/chapters/en/chapter6/3.mdx  \n",
              "1                  huggingface/peft/blob/main/docs/source/conceptual_guides/ia3.md  \n",
              "2     gradio-app/gradio/blob/main/guides/04_chatbots/01_creating-a-chatbot-fast.md  \n",
              "3               huggingface/pytorch-image-models/blob/main/docs/models/xception.md  \n",
              "4  huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(pd.DataFrame(outputs).head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents\n",
        "\n",
        "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
        "\n",
        "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
        "- **Groundedness:** can the question be answered from the given context?\n",
        "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practitioners.\n",
        "\n",
        "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
        "We also build a critique agent for this criteria:\n",
        "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
        "\n",
        "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
        "\n",
        "üí° ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
        "\n",
        "We now build and run these critique agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "05aSgTGs9jVO"
      },
      "outputs": [],
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b9tbk7ME9jVO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [02:30<00:00,  5.01s/it]\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(\n",
        "            llm,\n",
        "            question_groundedness_critique_prompt.format(\n",
        "                context=output[\"context\"], question=output[\"question\"]\n",
        "            ),\n",
        "        ),\n",
        "        \"relevance\": call_llm(\n",
        "            llm,\n",
        "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "        \"standalone\": call_llm(\n",
        "            llm,\n",
        "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oBWuOu1b9jVO",
        "outputId": "b32bacea-52f8-486a-96fe-5c188605c5a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What does setting return_offsets_mapping=True do when applying the tokenizer to inputs?</td>\n",
              "      <td>It provides the start and end offsets of each token in the original sentence.</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To which layers of a Transformer model are IA3 weights added?</td>\n",
              "      <td>IA3 weights are added to the outputs of the key and value layers, and to the input of the second feedforward layer in each transformer block.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What Python library is used to create the chat interface in the provided examples?</td>\n",
              "      <td>gradio</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What type of convolutional layers does the Xception neural network architecture use exclusively?</td>\n",
              "      <td>Depthwise separable convolution layers</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What function is used to convert logits into probabilities in the described pipeline?</td>\n",
              "      <td>SoftMax</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What is the default value of the temperature parameter in the generation payload?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What is the name of the new sentiment analysis task introduced for the e-commerce domain in the benchmark?</td>\n",
              "      <td>Allegro Reviews (AR)</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the expected validation accuracy after fine-tuning the model using the provided script?</td>\n",
              "      <td>99%</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Under what license are the Hugging Face Transformers Notebooks released?</td>\n",
              "      <td>Apache License, Version 2.0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What activation function does EfficientNet-Lite use to make it more suitable for mobile devices?</td>\n",
              "      <td>ReLU6</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What optimizer should be used with ORTSeq2SeqTrainingArguments for ONNX Runtime training?</td>\n",
              "      <td>adamw_ort_fused</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Where should you post questions for help from the Hugging Face community?</td>\n",
              "      <td>In the Hugging Face forum.</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>How many samples are used to build the calibration dataset for post-training static quantization in this example?</td>\n",
              "      <td>300 samples</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Which optimizer has the highest logprob_diff_var after two epochs of training?</td>\n",
              "      <td>PyTorch‚Äôs Adam</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>How does the project define fairness?</td>\n",
              "      <td>Fairness is defined as the equal treatment of all human beings, including monitoring and mitigating unwanted biases based on characteristics such as race, gender, disabilities, and sexual orientation.</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>What does the 'multi_class' parameter specify when used with multiclass targets?</td>\n",
              "      <td>The 'multi_class' parameter determines the type of configuration to use for multiclass targets, with options 'ovr' (one-vs-rest) and 'ovo' (one-vs-one).</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>What command-line argument should you use with torchrun to select the number of GPUs for training?</td>\n",
              "      <td>--nproc_per_node</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>What is the main focus of the quantitative evaluation methods discussed in the document?</td>\n",
              "      <td>How to implement quantitative evaluation methods alongside diffusers.</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What new types of pipelines can now be loaded using Interface.load in Gradio 3.17.0?</td>\n",
              "      <td>image-to-text and conversational pipelines</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What is the function used to perform a forward pass in Data2VecVisionForSemanticSegmentation?</td>\n",
              "      <td>forward</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>What is the shape of the logits output by the Kakao Brain ViT image classification model?</td>\n",
              "      <td>(batch_size, 1000)</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What is the name of the first convolution-free, purely attention-based model for audio classification introduced in the paper?</td>\n",
              "      <td>Audio Spectrogram Transformer (AST)</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>What special tokens does the tokenizer add to the sentence by default?</td>\n",
              "      <td>CLS and SEP tokens</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Which optimizer is used with a learning rate of 2e-5 and a weight decay rate of 0.01 in the example?</td>\n",
              "      <td>AdamWeightDecay</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Which command-line flag ensures that training runs are tracked on Weights and Biases?</td>\n",
              "      <td>report_to=\"wandb\"</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Which dataset is listed as \"Merged\" and has a link to \"pranjali97/french_translated_snli\"?</td>\n",
              "      <td>pranjali97/french_translated_snli</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>What is the Top 1 Accuracy of the gluon_resnet50_v1b model on the ImageNet dataset?</td>\n",
              "      <td>77.58%</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>What command is used to activate a Python virtual environment on Windows?</td>\n",
              "      <td>.\\gradio-env\\Scripts\\activate</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>What does the Bellman equation express the value of a state as?</td>\n",
              "      <td>The Bellman equation expresses the value of a state as the immediate reward plus the discounted value of the next state.</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>What is the filename of the quantized Vicuna-13b model after running the provided command?</td>\n",
              "      <td>Vicuna-13b-4bit-act-order.safetensors</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                          question  \\\n",
              "0                                          What does setting return_offsets_mapping=True do when applying the tokenizer to inputs?   \n",
              "1                                                                    To which layers of a Transformer model are IA3 weights added?   \n",
              "2                                               What Python library is used to create the chat interface in the provided examples?   \n",
              "3                                 What type of convolutional layers does the Xception neural network architecture use exclusively?   \n",
              "4                                            What function is used to convert logits into probabilities in the described pipeline?   \n",
              "5                                                What is the default value of the temperature parameter in the generation payload?   \n",
              "6                       What is the name of the new sentiment analysis task introduced for the e-commerce domain in the benchmark?   \n",
              "7                                  What is the expected validation accuracy after fine-tuning the model using the provided script?   \n",
              "8                                                         Under what license are the Hugging Face Transformers Notebooks released?   \n",
              "9                                 What activation function does EfficientNet-Lite use to make it more suitable for mobile devices?   \n",
              "10                                       What optimizer should be used with ORTSeq2SeqTrainingArguments for ONNX Runtime training?   \n",
              "11                                                       Where should you post questions for help from the Hugging Face community?   \n",
              "12               How many samples are used to build the calibration dataset for post-training static quantization in this example?   \n",
              "13                                                  Which optimizer has the highest logprob_diff_var after two epochs of training?   \n",
              "14                                                                                           How does the project define fairness?   \n",
              "15                                                What does the 'multi_class' parameter specify when used with multiclass targets?   \n",
              "16                              What command-line argument should you use with torchrun to select the number of GPUs for training?   \n",
              "17                                        What is the main focus of the quantitative evaluation methods discussed in the document?   \n",
              "18                                            What new types of pipelines can now be loaded using Interface.load in Gradio 3.17.0?   \n",
              "19                                   What is the function used to perform a forward pass in Data2VecVisionForSemanticSegmentation?   \n",
              "20                                       What is the shape of the logits output by the Kakao Brain ViT image classification model?   \n",
              "21  What is the name of the first convolution-free, purely attention-based model for audio classification introduced in the paper?   \n",
              "22                                                          What special tokens does the tokenizer add to the sentence by default?   \n",
              "23                            Which optimizer is used with a learning rate of 2e-5 and a weight decay rate of 0.01 in the example?   \n",
              "24                                           Which command-line flag ensures that training runs are tracked on Weights and Biases?   \n",
              "25                                      Which dataset is listed as \"Merged\" and has a link to \"pranjali97/french_translated_snli\"?   \n",
              "26                                             What is the Top 1 Accuracy of the gluon_resnet50_v1b model on the ImageNet dataset?   \n",
              "27                                                       What command is used to activate a Python virtual environment on Windows?   \n",
              "28                                                                 What does the Bellman equation express the value of a state as?   \n",
              "29                                      What is the filename of the quantized Vicuna-13b model after running the provided command?   \n",
              "\n",
              "                                                                                                                                                                                                      answer  \\\n",
              "0                                                                                                                              It provides the start and end offsets of each token in the original sentence.   \n",
              "1                                                              IA3 weights are added to the outputs of the key and value layers, and to the input of the second feedforward layer in each transformer block.   \n",
              "2                                                                                                                                                                                                     gradio   \n",
              "3                                                                                                                                                                     Depthwise separable convolution layers   \n",
              "4                                                                                                                                                                                                    SoftMax   \n",
              "5                                                                                                                                                                                                        1.0   \n",
              "6                                                                                                                                                                                       Allegro Reviews (AR)   \n",
              "7                                                                                                                                                                                                        99%   \n",
              "8                                                                                                                                                                                Apache License, Version 2.0   \n",
              "9                                                                                                                                                                                                      ReLU6   \n",
              "10                                                                                                                                                                                           adamw_ort_fused   \n",
              "11                                                                                                                                                                                In the Hugging Face forum.   \n",
              "12                                                                                                                                                                                               300 samples   \n",
              "13                                                                                                                                                                                            PyTorch‚Äôs Adam   \n",
              "14  Fairness is defined as the equal treatment of all human beings, including monitoring and mitigating unwanted biases based on characteristics such as race, gender, disabilities, and sexual orientation.   \n",
              "15                                                  The 'multi_class' parameter determines the type of configuration to use for multiclass targets, with options 'ovr' (one-vs-rest) and 'ovo' (one-vs-one).   \n",
              "16                                                                                                                                                                                          --nproc_per_node   \n",
              "17                                                                                                                                     How to implement quantitative evaluation methods alongside diffusers.   \n",
              "18                                                                                                                                                                image-to-text and conversational pipelines   \n",
              "19                                                                                                                                                                                                   forward   \n",
              "20                                                                                                                                                                                        (batch_size, 1000)   \n",
              "21                                                                                                                                                                       Audio Spectrogram Transformer (AST)   \n",
              "22                                                                                                                                                                                        CLS and SEP tokens   \n",
              "23                                                                                                                                                                                           AdamWeightDecay   \n",
              "24                                                                                                                                                                                         report_to=\"wandb\"   \n",
              "25                                                                                                                                                                         pranjali97/french_translated_snli   \n",
              "26                                                                                                                                                                                                    77.58%   \n",
              "27                                                                                                                                                                             .\\gradio-env\\Scripts\\activate   \n",
              "28                                                                                  The Bellman equation expresses the value of a state as the immediate reward plus the discounted value of the next state.   \n",
              "29                                                                                                                                                                     Vicuna-13b-4bit-act-order.safetensors   \n",
              "\n",
              "    groundedness_score  relevance_score  standalone_score  \n",
              "0                    4                5                 5  \n",
              "1                    5                4                 5  \n",
              "2                    5                3                 1  \n",
              "3                    5                1                 5  \n",
              "4                    5                4                 1  \n",
              "5                    5                4                 1  \n",
              "6                    5                3                 1  \n",
              "7                    5                2                 1  \n",
              "8                    5                4                 5  \n",
              "9                    5                2                 5  \n",
              "10                   5                4                 5  \n",
              "11                   5                2                 5  \n",
              "12                   5                4                 1  \n",
              "13                   5                2                 1  \n",
              "14                   5                3                 1  \n",
              "15                   5                4                 5  \n",
              "16                   5                5                 5  \n",
              "17                   2                3                 1  \n",
              "18                   5                4                 1  \n",
              "19                   5                5                 5  \n",
              "20                   5                3                 5  \n",
              "21                   5                3                 1  \n",
              "22                   5                5                 1  \n",
              "23                   5                3                 1  \n",
              "24                   5                4                 5  \n",
              "25                   5                2                 1  \n",
              "26                   5                1                 5  \n",
              "27                   5                2                 5  \n",
              "28                   5                2                 5  \n",
              "29                   5                2                 1  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================\n",
            "Final evaluation dataset:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What does setting return_offsets_mapping=True do when applying the tokenizer to inputs?</td>\n",
              "      <td>It provides the start and end offsets of each token in the original sentence.</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>To which layers of a Transformer model are IA3 weights added?</td>\n",
              "      <td>IA3 weights are added to the outputs of the key and value layers, and to the input of the second feedforward layer in each transformer block.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Under what license are the Hugging Face Transformers Notebooks released?</td>\n",
              "      <td>Apache License, Version 2.0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What optimizer should be used with ORTSeq2SeqTrainingArguments for ONNX Runtime training?</td>\n",
              "      <td>adamw_ort_fused</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>What does the 'multi_class' parameter specify when used with multiclass targets?</td>\n",
              "      <td>The 'multi_class' parameter determines the type of configuration to use for multiclass targets, with options 'ovr' (one-vs-rest) and 'ovo' (one-vs-one).</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>What command-line argument should you use with torchrun to select the number of GPUs for training?</td>\n",
              "      <td>--nproc_per_node</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>What is the function used to perform a forward pass in Data2VecVisionForSemanticSegmentation?</td>\n",
              "      <td>forward</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Which command-line flag ensures that training runs are tracked on Weights and Biases?</td>\n",
              "      <td>report_to=\"wandb\"</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              question  \\\n",
              "0              What does setting return_offsets_mapping=True do when applying the tokenizer to inputs?   \n",
              "1                                        To which layers of a Transformer model are IA3 weights added?   \n",
              "8                             Under what license are the Hugging Face Transformers Notebooks released?   \n",
              "10           What optimizer should be used with ORTSeq2SeqTrainingArguments for ONNX Runtime training?   \n",
              "15                    What does the 'multi_class' parameter specify when used with multiclass targets?   \n",
              "16  What command-line argument should you use with torchrun to select the number of GPUs for training?   \n",
              "19       What is the function used to perform a forward pass in Data2VecVisionForSemanticSegmentation?   \n",
              "24               Which command-line flag ensures that training runs are tracked on Weights and Biases?   \n",
              "\n",
              "                                                                                                                                                      answer  \\\n",
              "0                                                                              It provides the start and end offsets of each token in the original sentence.   \n",
              "1              IA3 weights are added to the outputs of the key and value layers, and to the input of the second feedforward layer in each transformer block.   \n",
              "8                                                                                                                                Apache License, Version 2.0   \n",
              "10                                                                                                                                           adamw_ort_fused   \n",
              "15  The 'multi_class' parameter determines the type of configuration to use for multiclass targets, with options 'ovr' (one-vs-rest) and 'ovo' (one-vs-one).   \n",
              "16                                                                                                                                          --nproc_per_node   \n",
              "19                                                                                                                                                   forward   \n",
              "24                                                                                                                                         report_to=\"wandb\"   \n",
              "\n",
              "    groundedness_score  relevance_score  standalone_score  \n",
              "0                    4                5                 5  \n",
              "1                    5                4                 5  \n",
              "8                    5                4                 5  \n",
              "10                   5                4                 5  \n",
              "15                   5                4                 5  \n",
              "16                   5                5                 5  \n",
              "19                   5                5                 5  \n",
              "24                   5                4                 5  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 4)\n",
        "    & (generated_questions[\"relevance_score\"] >= 4)\n",
        "    & (generated_questions[\"standalone_score\"] >= 4)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_dataset = datasets.Dataset.from_pandas(\n",
        "    generated_questions, split=\"train\", preserve_index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
        "\n",
        "We have generated only a few QA couples here to reduce time and cost. But let's kickstart the next part by loading a pre-generated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Q3RRz4W79jVO"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f27f471f1214e0c80c570925b098d37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e29ba328d3d434997b8db64109f3b07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8af0da888ab47aead666fcd51507be4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "datasets_local/20250913_120022\n"
          ]
        }
      ],
      "source": [
        "# save datasets locally\n",
        "\n",
        "import os, json, csv, datetime\n",
        "\n",
        "\n",
        "# helpers\n",
        "def to_serializable(x):\n",
        "    try:\n",
        "        json.dumps(x)\n",
        "        return x\n",
        "    except TypeError:\n",
        "        if hasattr(x, \"page_content\"):\n",
        "            return {\n",
        "                \"page_content\": getattr(x, \"page_content\", None),\n",
        "                \"metadata\": getattr(x, \"metadata\", {}),\n",
        "            }\n",
        "        if isinstance(x, dict):\n",
        "            return {k: to_serializable(v) for k, v in x.items()}\n",
        "        if isinstance(x, (list, tuple, set)):\n",
        "            return [to_serializable(i) for i in x]\n",
        "        if hasattr(x, \"__dict__\"):\n",
        "            return {k: to_serializable(v) for k, v in vars(x).items()}\n",
        "        return repr(x)\n",
        "\n",
        "\n",
        "def write_jsonl(path, records):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in records:\n",
        "            f.write(json.dumps(to_serializable(r), ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "\n",
        "def write_csv(path, records):\n",
        "    if not records or not isinstance(records[0], dict):\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            pass\n",
        "        return\n",
        "    keys = sorted({k for r in records for k in r.keys()})\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=keys)\n",
        "        w.writeheader()\n",
        "        for r in records:\n",
        "            w.writerow({k: r.get(k, \"\") for k in keys})\n",
        "\n",
        "\n",
        "# output directory\n",
        "save_dir = f\"datasets_local/{datetime.datetime.now():%Y%m%d_%H%M%S}\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# initial corpus (try common variable names)\n",
        "for name in (\"initial_corpus\", \"raw_corpus\", \"raw_docs\", \"input_docs\", \"docs_raw\"):\n",
        "    if name in globals():\n",
        "        write_jsonl(os.path.join(save_dir, \"initial_corpus.jsonl\"), globals()[name])\n",
        "        break\n",
        "\n",
        "# processed docs\n",
        "if \"docs_processed\" in globals():\n",
        "    write_jsonl(os.path.join(save_dir, \"processed_docs.jsonl\"), docs_processed)\n",
        "\n",
        "# qa datasets\n",
        "if \"outputs\" in globals():\n",
        "    write_jsonl(os.path.join(save_dir, \"qa_all.jsonl\"), outputs)\n",
        "    if outputs and isinstance(outputs[0], dict):\n",
        "        write_csv(os.path.join(save_dir, \"qa_all.csv\"), outputs)\n",
        "\n",
        "if \"outputs_filtered\" in globals():\n",
        "    write_jsonl(os.path.join(save_dir, \"qa_filtered.jsonl\"), outputs_filtered)\n",
        "    if outputs_filtered and isinstance(outputs_filtered[0], dict):\n",
        "        write_csv(os.path.join(save_dir, \"qa_filtered.csv\"), outputs_filtered)\n",
        "\n",
        "if \"outputs_rejected\" in globals():\n",
        "    write_jsonl(os.path.join(save_dir, \"qa_rejected.jsonl\"), outputs_rejected)\n",
        "\n",
        "print(save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load datasets from a saved folder\n",
        "\n",
        "import os, json\n",
        "\n",
        "\n",
        "def read_jsonl(path):\n",
        "    if not os.path.exists(path):\n",
        "        return []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f]\n",
        "\n",
        "\n",
        "# set folder produced by the save cell\n",
        "load_dir = \"datasets_local/20250913_120022\"\n",
        "\n",
        "initial_corpus = read_jsonl(os.path.join(load_dir, \"initial_corpus.jsonl\"))\n",
        "docs_processed = read_jsonl(os.path.join(load_dir, \"processed_docs.jsonl\"))\n",
        "outputs = read_jsonl(os.path.join(load_dir, \"qa_all.jsonl\"))\n",
        "outputs_filtered = read_jsonl(os.path.join(load_dir, \"qa_filtered.jsonl\"))\n",
        "outputs_rejected = read_jsonl(os.path.join(load_dir, \"qa_rejected.jsonl\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
