{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2647/2647 [00:00<00:00, 632358.76it/s]\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Import from helpers\n",
        "from rag_helpers import (\n",
        "    read_jsonl,\n",
        "    split_documents,\n",
        "    load_embeddings,\n",
        "    answer_with_rag,\n",
        "    RAG_PROMPT_TEMPLATE,\n",
        ")\n",
        "\n",
        "# Set folder produced by the save step\n",
        "load_dir = \"datasets_local/20250914_145157\"\n",
        "raw_path = os.path.join(load_dir, \"initial_corpus.jsonl\")\n",
        "\n",
        "ds = read_jsonl(raw_path)\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm.tqdm(ds)\n",
        "]\n",
        "\n",
        "# Setup Reader LLM\n",
        "READER_MODEL_NAME = \"gpt-4o-mini\"\n",
        "READER_LLM = ChatOpenAI(\n",
        "    model=READER_MODEL_NAME,\n",
        "    temperature=0.1,\n",
        "    max_tokens=512,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3RRz4W79jVO"
      },
      "outputs": [],
      "source": [
        "# # Load saved raw knowledge base into LangChain documents\n",
        "\n",
        "# import os, json, tqdm\n",
        "# from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "\n",
        "# def read_jsonl(path):\n",
        "#     if not os.path.exists(path):\n",
        "#         raise FileNotFoundError(f\"Missing file: {path}\")\n",
        "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "#         return [json.loads(line) for line in f]\n",
        "\n",
        "\n",
        "# # Set folder produced by the save step\n",
        "# load_dir = \"datasets_local/20250914_145157\"\n",
        "# raw_path = os.path.join(load_dir, \"initial_corpus.jsonl\")\n",
        "\n",
        "# ds = read_jsonl(raw_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
        "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
        "\n",
        "Many options exist for text splitting:\n",
        "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
        "- split after `n` words / character, but only on sentence boundaries\n",
        "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
        "\n",
        "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
        "\n",
        "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "üí° _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "H4fhm55Q9jVO"
      },
      "outputs": [],
      "source": [
        "# RAW_KNOWLEDGE_BASE = [\n",
        "#     LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "#     for doc in tqdm.tqdm(ds)\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "sz9Jw2_q9jVO"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from typing import List\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "# def split_documents(\n",
        "#     chunk_size: int,\n",
        "#     knowledge_base: List[LangchainDocument],\n",
        "#     tokenizer_name: str,\n",
        "# ) -> List[LangchainDocument]:\n",
        "#     \"\"\"\n",
        "#     Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
        "#     \"\"\"\n",
        "#     text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "#         encoding_name=tokenizer_name,\n",
        "#         chunk_size=chunk_size,\n",
        "#         chunk_overlap=int(chunk_size / 10),\n",
        "#         add_start_index=True,\n",
        "#         strip_whitespace=True,\n",
        "#         separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "#     )\n",
        "\n",
        "#     docs_processed = []\n",
        "#     for doc in knowledge_base:\n",
        "#         docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "#     # Remove duplicates\n",
        "#     unique_texts = {}\n",
        "#     docs_processed_unique = []\n",
        "#     for doc in docs_processed:\n",
        "#         if doc.page_content not in unique_texts:\n",
        "#             unique_texts[doc.page_content] = True\n",
        "#             docs_processed_unique.append(doc)\n",
        "\n",
        "#     return docs_processed_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings üóÇÔ∏è\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
        "\n",
        "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
        "\n",
        "üõ†Ô∏è __Options included:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "LqJlIDZR9jVO"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "from typing import Optional, List\n",
        "import os\n",
        "\n",
        "\n",
        "# def load_embeddings(\n",
        "#     langchain_docs: List[LangchainDocument],\n",
        "#     chunk_size: int,\n",
        "#     embedding_model_name: Optional[str] = \"text-embedding-3-small\",\n",
        "# ) -> FAISS:\n",
        "#     \"\"\"\n",
        "#     Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
        "\n",
        "#     Args:\n",
        "#         langchain_docs: list of documents\n",
        "#         chunk_size: size of the chunks to split the documents into\n",
        "#         embedding_model_name: name of the embedding model to use\n",
        "\n",
        "#     Returns:\n",
        "#         FAISS index\n",
        "#     \"\"\"\n",
        "#     # load embedding_model\n",
        "#     embedding_model = OpenAIEmbeddings(\n",
        "#         model=embedding_model_name,\n",
        "#     )\n",
        "\n",
        "#     # Check if embeddings already exist on disk\n",
        "#     index_name = (\n",
        "#         f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
        "#     )\n",
        "#     index_folder_path = f\"data/indexes/{index_name}/\"\n",
        "#     if os.path.isdir(index_folder_path):\n",
        "#         return FAISS.load_local(\n",
        "#             index_folder_path,\n",
        "#             embedding_model,\n",
        "#             allow_dangerous_deserialization=True,\n",
        "#         )\n",
        "\n",
        "#     else:\n",
        "#         print(\"Index not found, generating it...\")\n",
        "#         docs_processed = split_documents(\n",
        "#             chunk_size,\n",
        "#             langchain_docs,\n",
        "#             \"cl100k_base\",\n",
        "#         )\n",
        "#         knowledge_index = FAISS.from_documents(\n",
        "#             docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "#         )\n",
        "#         knowledge_index.save_local(index_folder_path)\n",
        "#         return knowledge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM üí¨\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
        "\n",
        "üõ†Ô∏è Here we tried the following options to improve results:\n",
        "- Switch reranking on/off\n",
        "- Change the reader model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "# RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "# <|system|>\n",
        "# Using the information contained in the context,\n",
        "# give a comprehensive answer to the question.\n",
        "# Respond only to the question asked, response should be concise and relevant to the question.\n",
        "# Provide the number of the source document when relevant.\n",
        "# If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "# <|user|>\n",
        "# Context:\n",
        "# {context}\n",
        "# ---\n",
        "# Now here is the question you need to answer.\n",
        "\n",
        "# Question: {question}\n",
        "# </s>\n",
        "# <|assistant|>\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "9SDqenld9jVP"
      },
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# READER_MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "# READER_LLM = ChatOpenAI(\n",
        "#     model=READER_MODEL_NAME,\n",
        "#     temperature=0.1,\n",
        "#     max_tokens=512,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QZ62CbcZ9jVP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.language_models.base import BaseLanguageModel\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "\n",
        "# def answer_with_rag(\n",
        "#     question: str,\n",
        "#     llm: BaseLanguageModel,\n",
        "#     knowledge_index: VectorStore,\n",
        "#     reranker: Optional[any] = None,\n",
        "#     num_retrieved_docs: int = 30,\n",
        "#     num_docs_final: int = 7,\n",
        "# ) -> Tuple[str, List[LangchainDocument]]:\n",
        "#     \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
        "#     # Gather documents with retriever\n",
        "#     relevant_docs = knowledge_index.similarity_search(\n",
        "#         query=question, k=num_retrieved_docs\n",
        "#     )\n",
        "#     relevant_docs_text = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "#     # Optionally rerank results\n",
        "#     if reranker:\n",
        "#         reranked = reranker.rerank(question, relevant_docs_text, k=num_docs_final)\n",
        "#         relevant_docs_text = [doc[\"content\"] for doc in reranked]\n",
        "\n",
        "#     relevant_docs_text = relevant_docs_text[:num_docs_final]\n",
        "\n",
        "#     # Build the final prompt\n",
        "#     context = \"\\nExtracted documents:\\n\"\n",
        "#     context += \"\".join(\n",
        "#         [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs_text)]\n",
        "#     )\n",
        "\n",
        "#     final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "#     # Redact an answer\n",
        "#     if hasattr(llm, \"invoke\"):\n",
        "#         response = llm.invoke(final_prompt)\n",
        "#         answer = response.content if hasattr(response, \"content\") else str(response)\n",
        "#     else:\n",
        "#         answer = llm(final_prompt)\n",
        "\n",
        "#     return answer, relevant_docs_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
